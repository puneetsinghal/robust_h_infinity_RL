{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/puneet/.virtualenvs/rl3/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/puneet/.virtualenvs/rl3/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from math import *\n",
    "import os\n",
    "import argparse\n",
    "from copy import copy\n",
    "\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "import pickle\n",
    "from IPython import embed\n",
    "from scipy import integrate as SCI_INT\n",
    "import h5py\n",
    "\n",
    "try:\n",
    "    from StringIO import StringIO\n",
    "except ImportError:\n",
    "    from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = False\n",
    "config.allow_soft_placement = True\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "SESS = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "M  = 20\n",
    "dt = 0.033\n",
    "DIM = 4\n",
    "\n",
    "tspan = np.arange(0,60,dt) \n",
    "\n",
    "T = tspan.size\n",
    "\n",
    "u = np.zeros(T)\n",
    "w = np.zeros(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_function(x, DIM):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    x3 = x[2]\n",
    "    x4 = x[3]\n",
    "\n",
    "    epsilon = 0.2  \n",
    "    D = 1-(epsilon*np.cos(x3))**2\n",
    "    g = np.array([0, -epsilon*np.cos(x3)/D, 0, 1/D]).reshape(DIM,1)\n",
    "    return g\n",
    "\n",
    "def k_function(x, DIM):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    x3 = x[2]\n",
    "    x4 = x[3]\n",
    "\n",
    "    epsilon = 0.2  \n",
    "    D = 1-(epsilon*np.cos(x3))**2\n",
    "\n",
    "    k = np.array([0, 1/D, 0, -epsilon*np.cos(x3)/D]).reshape(DIM,1)\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    \"\"\"Logging in tensorboard without tensorflow ops.\"\"\"\n",
    "\n",
    "    def __init__(self, log_dir):\n",
    "        \"\"\"Creates a summary writer logging to log_dir.\"\"\"\n",
    "        self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    def log_scalar(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\n",
    "        Parameter\n",
    "        ----------\n",
    "        tag : basestring\n",
    "            Name of the scalar\n",
    "        value\n",
    "        step : int\n",
    "            training iteration\n",
    "        \"\"\"\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag,\n",
    "                                                     simple_value=value)])\n",
    "        self.writer.add_summary(summary, step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_log_dir(log_parent_dir):\n",
    "    import datetime, os\n",
    "    current_timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    return current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    def __init__(self, params):\n",
    "        # tf Graph input\n",
    "        self.X_t = tf.placeholder(shape=[None, params['numState']], dtype=tf.float64, name=\"X_t\")\n",
    "        self.X_tPlus = tf.placeholder(shape=[None, params['numState']], dtype=tf.float64, name=\"X_tPlus\")\n",
    "        self.U_t = tf.placeholder(shape=[None, 1, params['action_size']], dtype=tf.float64, name=\"U_t\")\n",
    "        self.U_tPlus = tf.placeholder(shape=[None, 1, params['action_size']], dtype=tf.float64, name=\"U_tPlus\")\n",
    "        self.W_t = tf.placeholder(shape=[None, 1, params['disturbance_size']], dtype=tf.float64, name=\"W_t\")\n",
    "        self.W_tPlus = tf.placeholder(shape=[None, 1, params['disturbance_size']], dtype=tf.float64, name=\"W_tPlus\")\n",
    "#         self.Y = tf.placeholder(shape=[None, params['numOutput']], dtype=tf.float64, name=\"Y\")\n",
    "        \n",
    "        self.G_X_t = tf.placeholder(shape=[None, params['numState'], params['action_size']], dtype=tf.float64, name=\"g_x\")\n",
    "        self.G_X_tPlus = tf.placeholder(shape=[None, params['numState'], params['action_size']], dtype=tf.float64, name=\"g_x\")\n",
    "        self.K_X_t = tf.placeholder(shape=[None, params['numState'], params['disturbance_size']], dtype=tf.float64, name=\"k_x\")\n",
    "        self.K_X_tPlus = tf.placeholder(shape=[None, params['numState'], params['disturbance_size']], dtype=tf.float64, name=\"k_x\")\n",
    "    \n",
    "        self.gamma = params['gamma']\n",
    "        self.dt = params['dt']\n",
    "\n",
    "        self.generateNetwork(params)\n",
    "\n",
    "    def generateNetwork(self, params):\n",
    "        \n",
    "        self.input = tf.concat([self.X_t, self.X_tPlus],0)\n",
    "#         print(\"shape of final input: {}\".format(self.input.shape))\n",
    "        \n",
    "        self.layer = self.input\n",
    "        self.layer = layers.fully_connected(inputs=self.layer, \n",
    "                                            num_outputs=params['hiddenSize'], activation_fn=None)#tf.nn.relu)\n",
    "        self.layer = layers.fully_connected(inputs=self.layer, \n",
    "                                            num_outputs=params['hiddenSize'], activation_fn=None)#tf.nn.relu)\n",
    "#         self.layer = layers.fully_connected(inputs=self.layer, \n",
    "#                                             num_outputs=params['hiddenSize'], activation_fn=None)#tf.nn.relu)\n",
    "        self.output = layers.fully_connected(inputs=self.layer, \n",
    "                                             num_outputs=1, activation_fn=None)\n",
    "        \n",
    "        self.value_t, self.value_tPlus = tf.split(self.output, num_or_size_splits=2, axis=0)\n",
    "#         print(\"shape of value_t: {}\".format(self.value_t.shape))\n",
    "#         print(\"shape of value_tPlus: {}\".format(self.value_tPlus.shape))\n",
    "        \n",
    "        \n",
    "        self.value_grad_t = tf.gradients(self.value_t, self.X_t)[0]\n",
    "        self.value_grad_t = tf.expand_dims(self.value_grad_t, 1)\n",
    "#         print(\"shape of value_grad_t: {}\".format(self.value_grad_t.shape))\n",
    "        \n",
    "        self.value_grad_tPlus = tf.gradients(self.value_tPlus, self.X_tPlus)[0]\n",
    "        self.value_grad_tPlus = tf.expand_dims(self.value_grad_tPlus, 1)\n",
    "        \n",
    "        # Define loss and optimizer\n",
    "#         self.actor = -0.5*tf.matmul(self.G_X, self.value_grad, transpose_a=True)\n",
    "#         self.critic = (0.5*self.gamma**2)*tf.matmul(self.K_X, self.value_grad, transpose_a=True)\n",
    "\n",
    "        self.calculateResidualError()\n",
    "#         self.trainer = tf.train.AdamOptimizer(learning_rate=params['learningRate'])\n",
    "        \n",
    "        local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "#         print(local_vars)\n",
    "        \n",
    "#         try:\n",
    "#             self.batchsize = self.residual_error.shape[0].value\n",
    "#             self.gradient_vector = []\n",
    "#             for i in range(self.batchsize):\n",
    "#                 grad = tf.gradients(self.residual_error[i,0], local_vars)\n",
    "#                 self.gradient_vector.append([self.residual_error[i,0]*g, v] for g,v in grad])\n",
    "#             self.gradient_residual_error = tf.reduce_mean(self.gradient_vector)\n",
    "#         except:\n",
    "#             print(\"error\")\n",
    "#             self.gradient_residual_error = tf.gradients(self.residual_error[0,0], local_vars)\n",
    "        self.batch_gradients = []\n",
    "        self.batchsize = self.residual_error.shape[0].value\n",
    "        if(self.batchsize == None):\n",
    "            self.batchsize = 1\n",
    "            \n",
    "#         for i in range(self.batchsize):\n",
    "#             instance_grads_and_vars = tf.gradients(self.residual_error[i,0], local_vars)\n",
    "# #             instance_gradients = [self.residual_error[i,0]*grad/self.batchsize for grad, variable in instance_grads_and_vars]\n",
    "#             self.batch_gradients.append(instance_grads_and_vars)\n",
    "        trainer = tf.contrib.opt.NadamOptimizer(learning_rate=params['learningRate'])\n",
    "    \n",
    "        self.gradients = tf.gradients(self.residual_error, local_vars)\n",
    "#         self.gradients = tf.multiply(self.residual_error, self.gradients)\n",
    "#         self.gradients *= self.residual_error\n",
    "        self.var_norms = tf.global_norm(local_vars)\n",
    "        grads, self.grad_norms = tf.clip_by_global_norm(self.gradients, 300)\n",
    "        self.apply_grads = trainer.apply_gradients(zip(grads, local_vars))\n",
    "        \n",
    "#         self.gradient_residual_error = tf.gradients(self.residual_error, local_vars)\n",
    "#         print(\"shape of gradient_residual_error: {}\".format(self.gradient_residual_error))\n",
    "#         self.weighted_average = tf.reduce_mean(np.multiply(self.residual_error,self.gradient_residual_error),0)\n",
    "        \n",
    "#         self.trainer = tf.contrib.opt.NadamOptimizer(learning_rate=params['learningRate'])\n",
    "#         print(tf.reduce_sum(self.batch_gradients))\n",
    "#         print(zip(tf.reduce_sum(self.batch_gradients), local_vars))\n",
    "#         self.apply_grads = self.trainer.apply_gradients(zip(tf.reduce_sum(self.batch_gradients), local_vars))\n",
    "\n",
    "    def calculateResidualError(self):\n",
    "#         sigma = 0.0\n",
    "        actor_t = -0.5*tf.matmul(self.value_grad_t, self.G_X_t)\n",
    "        actor_tPlus = -0.5*tf.matmul(self.value_grad_tPlus, self.G_X_tPlus)\n",
    "#         print(\"shape of actor_tPlus: {}\".format(actor_tPlus.shape))\n",
    "        \n",
    "        firstTerm = -2.0*tf.reduce_sum(tf.multiply(actor_t, (self.U_t - actor_t)),2)\n",
    "        firstTerm += -2.0*tf.reduce_sum(tf.multiply(actor_tPlus, (self.U_tPlus - actor_tPlus)),2)\n",
    "        firstTerm *= self.dt\n",
    "        firstTerm /= 2.0\n",
    "#         print(\"shape of firstTerm: {}\".format(firstTerm.shape))\n",
    "        \n",
    "        critic_t = 0.5*tf.matmul(self.value_grad_t, self.K_X_t)\n",
    "#         critic_t = tf.transpose(critic_t)\n",
    "        critic_tPlus = 0.5*tf.matmul(self.value_grad_tPlus, self.K_X_tPlus)\n",
    "#         critic_tPlus = tf.transpose(critic_tPlus)\n",
    "        \n",
    "        secondTerm = 2.0*tf.reduce_sum(tf.multiply(critic_t, (self.W_t - critic_t)), 2)\n",
    "        secondTerm += 2.0*tf.reduce_sum(tf.multiply(critic_tPlus, (self.W_tPlus - critic_tPlus)), 2)\n",
    "        secondTerm *= self.dt\n",
    "        secondTerm /= 2.0\n",
    "#         print(\"shape of secondTerm: {}\".format(secondTerm.shape))\n",
    "        \n",
    "        thirdTerm = self.value_t - self.value_tPlus\n",
    "#         print(\"shape of thirdTerm: {}\".format(thirdTerm.shape))\n",
    "        \n",
    "        h_t = tf.expand_dims(sqrt(0.1)*self.X_t,1)\n",
    "        h_tPlus = tf.expand_dims(sqrt(0.1)*self.X_tPlus,1)\n",
    "        \n",
    "        fourthTerm = tf.reduce_sum(tf.multiply(h_t, h_t), 2)\n",
    "        fourthTerm += tf.reduce_sum(tf.multiply(h_tPlus, h_tPlus), 2)\n",
    "        fourthTerm *= self.dt\n",
    "        fourthTerm /= 2.0\n",
    "#         print(\"shape of fourthTerm: {}\".format(fourthTerm.shape))\n",
    "        \n",
    "        fifthTerm = tf.reduce_sum(tf.multiply(actor_t, actor_t),2)\n",
    "        fifthTerm += tf.reduce_sum(tf.multiply(actor_tPlus, actor_tPlus),2)\n",
    "        fifthTerm *= self.dt\n",
    "        fifthTerm /= 2.0\n",
    "#         print(\"shape of fifthTerm: {}\".format(fifthTerm.shape))\n",
    "        \n",
    "        sixthTerm = tf.reduce_sum(tf.multiply(critic_t, critic_t),2)\n",
    "        sixthTerm += tf.reduce_sum(tf.multiply(critic_tPlus, critic_tPlus),2)\n",
    "        sixthTerm *= self.dt\n",
    "        sixthTerm /= 2.0\n",
    "        sixthTerm *= self.gamma**2\n",
    "#         print(\"shape of sixthTerm: {}\".format(sixthTerm.shape))\n",
    "        \n",
    "        self.residual_error = firstTerm + secondTerm + thirdTerm - (fourthTerm + fifthTerm - sixthTerm)\n",
    "        self.residual_error = tf.square(self.residual_error)\n",
    "#         print(\"shape of residual_error: {}\".format(self.residual_error.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamicsknown(t, x):\n",
    "    # embed()\n",
    "    ut = np.interp(t, tspan, u)\n",
    "    wt = np.interp(t, tspan, w)\n",
    "\n",
    "    epsilon = 0.2\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    x3 = x[2]\n",
    "    x4 = x[3]\n",
    "    # embed()\n",
    "\n",
    "    D = 1. - (epsilon*np.cos(x3))**2\n",
    "    f = np.array([x2, (-x1+epsilon*x4**2*np.sin(x3))/D, x4, epsilon*np.cos(x3)*(x1 - epsilon*x4**2*np.sin(x3))/D])\n",
    "    g = np.array([0, -epsilon*np.cos(x3)/D, 0, 1/D])\n",
    "    k = np.array([0, 1/D, 0, -epsilon*np.cos(x3)/D])\n",
    "    # embed()\n",
    "\n",
    "    # ut   = -0.5*g.T*JsigmaL(x).T*theta_current\n",
    "\n",
    "    xdot = f + g*ut + k*wt\n",
    "    return xdot\n",
    "\n",
    "def generateSystemData(control_generator, T, M, DIM):\n",
    "    U = np.zeros([T*M,1])\n",
    "    W = np.zeros([T*M,1])\n",
    "    X = np.zeros([T*M,DIM])\n",
    "    x = np.zeros([T, DIM])\n",
    "    # embed()\n",
    "\n",
    "    SM = np.random.randint(0, 80, M)\n",
    "    for k in range(M):\n",
    "\n",
    "        x0 = np.random.rand(DIM)\n",
    "\n",
    "        j = SM[k]\n",
    "        \n",
    "        u      = 0.1*control_generator[:,j]\n",
    "        w      = 0.2*np.random.rand(T)\n",
    "        x[0,:] = x0\n",
    "\n",
    "        r = SCI_INT.ode(dynamicsknown).set_integrator(\"dopri5\") \n",
    "        r.set_initial_value(x0, 0)\n",
    "        for i in range(1, T):\n",
    "            # embed()\n",
    "            x[i, :] = r.integrate(r.t+dt)    # get one more value, add it to the array\n",
    "            if not r.successful():\n",
    "                raise RuntimeError(\"Could not integrate\")\n",
    "        # embed()\n",
    "        U[k*T:(k+1)*T,0] = u\n",
    "        W[k*T:(k+1)*T,0] = w\n",
    "        X[k*T:(k+1)*T,:] = x\n",
    "\n",
    "        print(j)\n",
    "\n",
    "    file = h5py.File('systemData3.h5', 'w') \n",
    "    file.create_dataset('U', data=U)\n",
    "    file.create_dataset('W', data=W)\n",
    "    file.create_dataset('X', data=X)\n",
    "    file.close()\n",
    "\n",
    "    return U, W, X\n",
    "\n",
    "def generateInputFunction(tspan):\n",
    "    T = tspan.size\n",
    "    I1 = np.identity(T)\n",
    "    KK = np.linspace(1,20,20,True).reshape(1,20)\n",
    "\n",
    "    control_generator = I1[:,0].reshape(T,1)*KK\n",
    "    control_generator = np.hstack((control_generator, -I1[:,0].reshape(T,1)*KK))\n",
    "    control_generator = np.hstack((control_generator, np.random.rand(T,1)))\n",
    "    control_generator = np.hstack((control_generator, np.sin(np.pi*tspan.reshape(T,1)*KK)))\n",
    "    control_generator = np.hstack((control_generator, np.cos(np.pi*tspan.reshape(T,1)*KK)))\n",
    "    control_generator = np.hstack((control_generator, np.exp(-np.pi*tspan.reshape(T,1)*KK)))\n",
    "\n",
    "    return control_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    params = {}\n",
    "    params['hiddenSize'] = 16\n",
    "    params['dt'] = 0.033\n",
    "    params['learningRate'] = 1e-2\n",
    "    params['gamma'] = 6\n",
    "    params['numState'] = DIM\n",
    "    params['action_size'] = 1\n",
    "    params['disturbance_size'] = 1\n",
    "    \n",
    "    nn = network(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "74\n",
      "26\n",
      "31\n",
      "24\n",
      "73\n",
      "19\n",
      "15\n",
      "51\n",
      "55\n",
      "75\n",
      "5\n",
      "56\n",
      "78\n",
      "20\n",
      "24\n",
      "19\n",
      "79\n",
      "65\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "#     L = sigmaL(np.zeros(DIM)).size\n",
    "\n",
    "control_generator = generateInputFunction(tspan)\n",
    "\n",
    "U, W, X = generateSystemData(control_generator, T, M, DIM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = np.zeros([M*(T-1), DIM])\n",
    "X_tPlus = np.zeros([M*(T-1), DIM])\n",
    "U_t = np.zeros([M*(T-1), 1, 1])\n",
    "U_tPlus = np.zeros([M*(T-1), 1, 1])\n",
    "W_t = np.zeros([M*(T-1), 1, 1])\n",
    "W_tPlus = np.zeros([M*(T-1), 1, 1])\n",
    "\n",
    "for k in range(M):\n",
    "    X_t[k*(T-1):(k+1)*(T-1), :] = X[k*T:(k+1)*T-1, :]\n",
    "    X_tPlus[k*(T-1):(k+1)*(T-1), :] = X[k*T+1:(k+1)*T, :]\n",
    "    U_t[k*(T-1):(k+1)*(T-1), 0, :] = U[k*T:(k+1)*T-1, :]\n",
    "    U_tPlus[k*(T-1):(k+1)*(T-1), 0, :] = U[k*T+1:(k+1)*T, :]\n",
    "    W_t[k*(T-1):(k+1)*(T-1), 0, :] = W[k*T:(k+1)*T-1, :]\n",
    "    W_tPlus[k*(T-1):(k+1)*(T-1), 0, :] = W[k*T+1:(k+1)*T, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_X_t = np.zeros([M*(T-1), DIM, 1])\n",
    "G_X_tPlus = np.zeros([M*(T-1), DIM, 1])\n",
    "K_X_t = np.zeros([M*(T-1), DIM, 1])\n",
    "K_X_tPlus = np.zeros([M*(T-1), DIM, 1])\n",
    "for j in range(M*(T-1)):\n",
    "    G_X_t[j, :, :] = g_function(X_t[j,:], DIM)\n",
    "    G_X_tPlus[j, :, :] = g_function(X_tPlus[j,:], DIM)\n",
    "    K_X_t[j, :, :] = k_function(X_t[j,:], DIM)\n",
    "    K_X_tPlus[j, :, :] = k_function(X_tPlus[j,:], DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "SESS.run(init)\n",
    "save_path = saver.save(SESS, './')\n",
    "log_dir = './train_log/'\n",
    "\n",
    "logger = Logger('./train_log/')\n",
    "\n",
    "filename = 'data_NN_pickleData'\n",
    "modelName =  'model_NN'\n",
    "\n",
    "    \n",
    "feed_dict = {nn.X_t:X_t, nn.X_tPlus:X_tPlus, nn.U_t:U_t, nn.U_tPlus:U_tPlus, nn.W_t:W_t, nn.W_tPlus:W_tPlus,\n",
    "            nn.G_X_t:G_X_t, nn.G_X_tPlus:G_X_tPlus, nn.K_X_t:K_X_t, nn.K_X_tPlus:K_X_tPlus}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.476917955563847\n",
      "4.8437278316667225\n",
      "4.843274125951381\n",
      "4.843479166803874\n"
     ]
    }
   ],
   "source": [
    "for step in range(10000):\n",
    "    _, residual_error = SESS.run([nn.apply_grads, nn.residual_error], feed_dict=feed_dict)\n",
    "    average_error = np.sum(residual_error)/(T*M)\n",
    "    logger.log_scalar(tag='cost',value=average_error, step=step)\n",
    "    if(step%1000 == 0):\n",
    "        print(average_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamics(t, x):\n",
    "    \n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    x3 = x[2]\n",
    "    x4 = x[3]\n",
    "\n",
    "    epsilon = 0.2 ; \n",
    "    D = 1. - (epsilon*np.cos(x3))**2\n",
    "    f = np.array([x2, (-x1+epsilon*x4**2*np.sin(x3))/D, x4, epsilon*np.cos(x3)*(x1 - epsilon*x4**2*np.sin(x3))/D])\n",
    "    g = np.array([0, -epsilon*np.cos(x3)/D, 0, 1/D])\n",
    "    k = np.array([0, 1/D, 0, -epsilon*np.cos(x3)/D])\n",
    "    \n",
    "    grad = SESS.run(nn.value_grad_t,feed_dict={nn.X_t:x.reshape(1,DIM), nn.X_tPlus:x.reshape(1,DIM)})\n",
    "    u   = -0.5*np.matmul(g.reshape(1,4), grad.reshape(DIM,1))\n",
    "    d   = 0.0*np.exp(-0.1*t)*np.sin(t)\n",
    "#     print(u)\n",
    "    xdot = f + g*u + k*d\n",
    "#     print(xdot)\n",
    "    return xdot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0     = np.random.rand(DIM)\n",
    "\n",
    "newT = 100\n",
    "dt = 0.033\n",
    "\n",
    "totalPoints = int(newT/dt)\n",
    "\n",
    "x = np.zeros([totalPoints,4])\n",
    "x[0,:] = x0\n",
    "\n",
    "r = SCI_INT.ode(dynamics).set_integrator(\"dopri5\") \n",
    "r.set_initial_value(x0, 0)\n",
    "for i in range(1, totalPoints):\n",
    "    # embed()\n",
    "    x[i, :] = r.integrate(r.t+dt)    # get one more value, add it to the array\n",
    "    if not r.successful():\n",
    "        raise RuntimeError(\"Could not integrate\")\n",
    "\n",
    "t = np.linspace(0,newT,totalPoints)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(t,x[:,0],'-b','linewidth',1.8)\n",
    "plt.plot(t,x[:,1],'-r','linewidth',1.8)\n",
    "plt.plot(t,x[:,2],'-m','linewidth',1.8)\n",
    "plt.plot(t,x[:,3],'-k','linewidth',1.8)\n",
    "\n",
    "# j = legend('$x_1$','$x_2$','$x_3$','$x_4$');\n",
    "# set(j,'interpreter','latex','fontsize',28)\n",
    "# grid on\n",
    "# xlabel('Time [s]','interpreter','latex','fontsize',28);\n",
    "# ylabel('States','interpreter','latex','fontsize',28);\n",
    "# title('$H_{\\infty}$ stabilization of RTAC-nonlinear Problem','interpreter','latex','fontsize',32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
